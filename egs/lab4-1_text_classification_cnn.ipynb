{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification using CNNs\n",
    "\n",
    "`CNNs(Convolutional Neural Networs)` 구조를 이용하여 문장 분류를 해보도록 하겠습니다. 데이터는 `Pang & Lee (2004)`에서 사용한 subjectivity dataset을 이용하였습니다. (http://www.cs.cornell.edu/people/pabo/movie-review-data/)\n",
    "\n",
    "이 노트북에 사용된 코드는 [이 곳](https://github.com/PacktPublishing/Deep-Learning-with-Keras/blob/master/Chapter05/finetune_word2vec_embeddings.py)에서 가지고 온 코드이며, 목적에 맞게 수정을 거쳤습니다.\n",
    "\n",
    "## Data\n",
    "\n",
    "**subjectivity** dataset은 5000개의 subjectivity 문장과 5000개의 objectivity 문장으로 구성되어 있습니다. subjectivity 문장들은 [Rotten Tomatoes](http://www.rottentomatoes.com)에 남겨진 영화 리뷰에서 가지고 온 문장들이며, objectivity 문장들은 [IMDB](http://www.imdb.com)에서 요약된 영화 줄거리에서 가지고 온 문장입니다. 실제 데이터를 수집한 저자들은 대부분의 경우 해당 레이블이 일치하였으나, 영화 줄거리 중 몇 개의 데이터가 실제로는 subjectivity일 수도 있다고 설명합니다. \n",
    "\n",
    "lab4-0 notebook을 이용하여 두 개의 텍스트 파일을 불러와서 **`data/subjectivity/massaged_subjectivity.txt`** 파일로 저장하였습니다. 해당 파일은 **label'\\t'sentence**의 형태로 두 개의 텍스트 파일을 합쳐놓았습니다. \n",
    "\n",
    "\n",
    "## Convolutional Neural Networs for Sentence Classification\n",
    "\n",
    "`CNN` 모델을 이용하여 텍스트를 분류하는 것은 `Yoon (2014)`(https://arxiv.org/abs/1408.5882) 에 잘 설명이 되어있습니다. 아래 그림을 통해 간략하게 구조를 살펴보겠습니다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](resources/cnn_structure.png \"CNN structure in Yoon (2014)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN은 인접한 계층의 연결성을 활용하여서 이미지의 공간 구조를 파악하였습니다. 문장 분류에서도 마찬가지입니다. 인접 계층의 연결성을 활용하여서 단어간의 공간 구조를 이용하여 문맥 정보를 유지하고, 그를 바탕으로 문장 분류를 하는 것입니다. 이번에는 `Yoon (2014)`의 구조와 유사하지만 훨씬 간단한 구조를 만들어서 살펴보도록 하겠습니다. \n",
    "\n",
    "기존의 **language model**에서는 `n-gram` 모델을 활용하여 단어의 문맥 구조를 파악하였습니다. 지금 사용할 CNN에서도 이와 유사한 과정이 있습니다. 한 번에 전체를 보는 것이 아니라 적절한 `필터 크기/커널 크기(filter/kernel size)`를 부여하여서 해당 범위 안의 단어를 살펴보는 `convolution filter`를 학습하게 되고, `max-pooling`을 통해서 문장에서 가장 중요한 특징(feature)을 나타내는 벡터를 얻게 됩니다. \n",
    "\n",
    "이미지의 경우 크기가 동일한 데이터를 많이 구할 수 있지만, 텍스트 데이터의 경우 문장의 길이를 특정짓기가 매우 어렵습니다. 그렇기 때문에 대부분의 경우 가장 긴 문장의 길이만큼의 배열로 설정한 이후, 문장의 길이가 짧을 경우 남은 공간을 `zero padding`을 하거나 `unknown word`로 처리하게 됩니다. \n",
    "\n",
    "이전에 `word embedding`에서와 마찬가지로 단어들은 각각의 index로 변환되어야 합니다. index로 변환된 단어들은 해당 단어의 벡터값으로 변환된 후, 입력값으로 사용됩니다. 입력값은 `Convolution layer`로 연결된 이후, `filter/kernel size`에 해당하는 단어들의 값을 미리 설정한 `conv_dim`만큼의 다른 방법으로 연산합니다. 각각의 `conv_dim`의 값들은 `max-pooling`의 과정을 통해 각각 하나의 값을 산출하고, `Fully connected layer`의 구조로 `구분할 클래스의 갯수`만큼의 노드로 출력하게 됩니다. `softmax` 활성화를 하게 되면 출력값은 확률로 변화되는데, 하나는 **subjectivity**, 다른 하나는 **objectivity**에 해당하는 값입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "\n",
    "필요한 모듈을 불러오고, 재실행을 대비하여 랜덤 시드를 설정하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.pooling import GlobalMaxPooling1D\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import codecs\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk를 불러온 이후, word_tokenizer를 실행하기 위해서는 `punkt` 패키지가 필요합니다. 아래 셀을 실행하여서 다운받겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 -m nltk.downloader -d /usr/local/share/nltk_data -u https://pastebin.com/raw/D3TBY4Mj punkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "필요한 파라미터들을 설정하도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"data/subjectivity/massaged_subjectivity.txt\"\n",
    "model = \"data/GoogleNews-vectors-negative300.bin\"\n",
    "embed_dim = 300  # word2vec 모델에서 설정된 embedding 차원의 값입니다.\n",
    "hidden_dim = 256 # Filter를 몇 개의 방법으로 연산할지 정합니다\n",
    "filter_size = 3  # Filter/kernel size 값입니다\n",
    "batch_size = 64  # 한 번에 몇 개의 데이터를 처리할 지 정합니다\n",
    "num_epoch = 10   # 전체 데이터를 몇 번 반복해서 학습할지 정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other parameters\n",
    "\n",
    "위에서 설정한 파라미터 이외에도 다음의 정보가 필요합니다. 해당 값들은 임의로 설정할 수 없고, 데이터를 분석하여 산출하여야 합니다. \n",
    " 1. 사용된 단어의 수\n",
    " 2. 단어-인덱스 관계를 정의한 `<dict>`\n",
    " 3. 인덱스-단어 관계를 정의한 `<dict>`\n",
    " \n",
    "먼저 단어의 갯수를 세어보도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22601\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "## Count the number of words\n",
    "counter = collections.Counter()\n",
    "f = open(data_file, \"rb\")\n",
    "seq_len = 0\n",
    "for line in f:\n",
    "    line = line.decode(\"ascii\", \"ignore\")\n",
    "    _, sent = line.strip().split(\"\\t\")\n",
    "    words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
    "    if len(words) > seq_len:\n",
    "        seq_len = len(words)\n",
    "    for word in words:\n",
    "        counter[word] += 1\n",
    "f.close()\n",
    "\n",
    "print(len(list(counter.keys())))\n",
    "print(seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용할 데이터에는 총 22,601개의 단어가 있다는 것을 볼 수 있습니다. 그리고 가장 긴 문장은 122개의 단어가 사용되었습니다. \n",
    "\n",
    "다음으로 `단어-인덱스` 관계를 정의하는 `<dict>`를 생성하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22602\n"
     ]
    }
   ],
   "source": [
    "word2idx = collections.defaultdict(int)\n",
    "for wid, word in enumerate(counter):\n",
    "    # print(wid, word)\n",
    "    word2idx[word] = wid + 1\n",
    "num_words = len(word2idx) + 1\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막에 하나를 더해주는 것은 지금 다루는 데이터에서는 괜찮지만 나중에 혹시나 사전에 등록되어있지 않은 단어를 예상해야할 경우를 대비하는 것입니다. \n",
    "\n",
    "다음으로 간단하게 `인덱스-단어` 관계를 정의하는 `<dict>`를 생성하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22602\n"
     ]
    }
   ],
   "source": [
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "print(len(idx2word) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "다음으로 사용할 데이터를 준비해보도록 하겠습니다. \n",
    "\n",
    "문장의 단어들은 각각 `word2idx`를 이용하여 인덱스로 변환될 것이며, 각 문장의 **subjectivity**를 뜻하는 `label`은 **np_utils**를 이용하여 `one-hot encoding`으로 변경할 것입니다. \n",
    "\n",
    "문장의 길이가 가장 긴 문장의 길이(122)보다 짧을 경우, 빈 공간을 `padding` 처리 하도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "f = open(data_file, \"rb\")\n",
    "for line in f:\n",
    "    line = line.decode(\"ascii\", \"ignore\")\n",
    "    label, sent = line.strip().split(\"\\t\")\n",
    "    y.append(int(label))\n",
    "    words = [x.lower() for x in nltk.word_tokenize(sent)]\n",
    "    wids = [word2index[word] for word in words]\n",
    "    X.append(wids)\n",
    "f.close()\n",
    "X = pad_sequences(X, maxlen=seq_len)\n",
    "y = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` 모듈에는 training/test 데이터를 분리해주는 함수가 포함되어 있습니다. 해당 함수를 사용하여서 데이터를 무작위로 섞은 이후, 학습용 데이터와 테스트용 데이터로 구분하도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 122) (3000, 122) (7000, 2) (3000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \n",
    "                                                random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "총 10,000개의 데이터 중 **7,000개**는 학습용, **3,000**개는 테스트용으로 분리된 것을 확인할 수 있습니다. 문장의 경우 `padding`을 통해 길이를 *122*개로 통일하였으며, label의 경우 `subjectivity/objectivity`의 *2*개로 표현된 것을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec & embedding layer\n",
    "\n",
    "이전에 살펴보았던 `Google word2vec` 모델을 이용하여 이번 notebook을 진행할 것입니다. 해당 모델을 **word2vec**이라는 변수 이름으로 불러왔습니다. \n",
    "\n",
    "**embedding layer**는 단어의 인덱스를 이용하여 각각의 단어를 **300개**의 *word embedding*으로 변경해주는 역할을 합니다. word embedding은 모델이 학습되는 과정에서 조금씩 변화하게 됩니다. embedding_layer를 만드는 방법은 일단 **전체 단어 크기 x word embedding 크기**의 zero-array를 만든 이후, 각 단어의 인덱스에 해당하는 행을 word embedding 값으로 변화시키는 것입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word2vec model\n",
    "word2vec = KeyedVectors.load_word2vec_format(model, binary=True)\n",
    "embedding_layer = np.zeros((num_words, embed_dim))\n",
    "for word, idx in word2idx.items():\n",
    "    try:\n",
    "        embedding_layer[idx, :] = word2vec[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model\n",
    "\n",
    "실제 학습을 할 모델의 구조를 만들어 보겠습니다. 먼저 데이터를 입력받은 이후(word index) 이를 word embedding 값으로 변환해줍니다. 이후 오버피팅을 방지하기 위하여 dropout 레이어를 추가해준 이후, `합성곱 레이어 (Convolutional layer)`를 생성합니다. `max pooling`을 통해 문장에서 가장 중요한 값을 `hidden_dim` 갯수만큼 추출한 이후, `fully connected layer (Dense layer)`의 형식으로 `one-hot encoding` 형태의 output으로 출력합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 122, 300)          6780600   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 122, 300)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 120, 256)          230656    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 7,011,770\n",
      "Trainable params: 7,011,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words, embed_dim, input_length=seq_len,\n",
    "                    weights=[embedding_layer],\n",
    "                    trainable=True))\n",
    "\n",
    "# output => batch size * seq_len * embed_dim\n",
    "\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "model.add(Conv1D(filters=hidden_dim, kernel_size=filter_size, activation=\"relu\"))\n",
    "\n",
    "# output => hidden_dim number of layers, with filter_size kernel. \n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "이제는 모델을 실제로 학습시켜보겠습니다. 이전과 다르게 `model.fit`의 결과를 다른 변수에 할당하는 것은 이후에 실제 그래프로 학습 과정을 확인하기 위해서입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/10\n",
      "7000/7000 [==============================] - 46s 7ms/step - loss: 0.3032 - acc: 0.8823 - val_loss: 0.2004 - val_acc: 0.9197\n",
      "Epoch 2/10\n",
      "7000/7000 [==============================] - 48s 7ms/step - loss: 0.1149 - acc: 0.9616 - val_loss: 0.1902 - val_acc: 0.9283\n",
      "Epoch 3/10\n",
      "7000/7000 [==============================] - 50s 7ms/step - loss: 0.0425 - acc: 0.9903 - val_loss: 0.1913 - val_acc: 0.9303\n",
      "Epoch 4/10\n",
      "7000/7000 [==============================] - 50s 7ms/step - loss: 0.0155 - acc: 0.9981 - val_loss: 0.2071 - val_acc: 0.9307\n",
      "Epoch 5/10\n",
      "7000/7000 [==============================] - 48s 7ms/step - loss: 0.0069 - acc: 0.9997 - val_loss: 0.2180 - val_acc: 0.9310\n",
      "Epoch 6/10\n",
      "7000/7000 [==============================] - 46s 7ms/step - loss: 0.0034 - acc: 0.9999 - val_loss: 0.2348 - val_acc: 0.9257\n",
      "Epoch 7/10\n",
      "7000/7000 [==============================] - 46s 7ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.2420 - val_acc: 0.9307\n",
      "Epoch 8/10\n",
      "7000/7000 [==============================] - 46s 7ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.2512 - val_acc: 0.9283\n",
      "Epoch 9/10\n",
      "7000/7000 [==============================] - 46s 7ms/step - loss: 9.9723e-04 - acc: 1.0000 - val_loss: 0.2589 - val_acc: 0.9267\n",
      "Epoch 10/10\n",
      "7000/7000 [==============================] - 46s 7ms/step - loss: 7.1016e-04 - acc: 1.0000 - val_loss: 0.2652 - val_acc: 0.9280\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = batch_size, epochs = num_epoch, validation_data = (X_test, y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 결과로 `training data`에 대해서는 100% 정확도를 보였으며, `test data`에 대해서는 대략 **93%**의 정확도를 보였음을 알 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look into the training process\n",
    "\n",
    "`python`은 `matplotlib` 이라는 모듈을 통해 데이터를 이용, 그래프를 그릴 수 있는 기능을 제공합니다. 앞서 `history` 변수에 저장된 내용을 토대로 실제 학습 과정에서 `loss` 및 `accuracy`가 어떻게 변화하였는지 확인해볼 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot accuracy\n",
    "plt.subplot(211)\n",
    "plt.title(\"accuracy\")\n",
    "plt.plot(history.history[\"acc\"], color=\"r\", label=\"train\")\n",
    "plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "# plot loss function\n",
    "plt.subplot(212)\n",
    "plt.title(\"loss\")\n",
    "plt.plot(history.history[\"loss\"], color=\"r\", label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"validation\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프를 보았을 때, `오버피팅(over-fitting)`이 발생한 것을 볼 수 있습니다. `hyperparameter` 수정이나 `regularization` 등의 방법을 통해서 오버피팅을 더욱 효과적으로 방지할 수 있을 것이라고 생각되며, filter_size의 경우에도 `Yoon (2014)` 논문에서는 하나의 filter_size를 사용한 것이 아니라 세 가지 다른 filter_size를 사용한 결과를 합쳐서 결과를 내는 방법을 선택했습니다. 지금은 이정도로 마무리짓고 넘어가겠습니다. 관심있으신 분들은 [Yoon (2014)](https://arxiv.org/abs/1408.5882) 논문을 참고하시고 실제로 구현해보셔도 좋을 것 같습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "마지막으로 모델의 결과를 살펴보도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 3s 1ms/step\n",
      "Test score: 0.265, accuracy: 0.928\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test score: {:.3f}, accuracy: {:.3f}\".format(score[0], score[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
