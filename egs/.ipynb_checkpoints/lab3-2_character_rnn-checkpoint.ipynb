{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "**Note:** 이 `notebook`을 실행하기 이전에 다음과 같은 내용을 설명하고 진행하도록 하겠습니다. \n",
    " 1. Recurrent Neural Networks의 구조\n",
    " 2. 여러가지 다른 형태의 RNNs 설명\n",
    "   - 다대다 (각각의 cell이 입력과 출력을 하는 경우)\n",
    "   - 다대다 (seq2seq: machine translation)\n",
    "   - 일대다 (img2seq)\n",
    "   - 다대일 (text classification, sentiment analysis)\n",
    "  \n",
    "**Note:** 이 `notebook`의 설명과 코드는 **Deep Learning with Keras** 책을 참고하였습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에는 `순환 신경망 (Recurrent Neural Networks; RNNs)`을 이용하여서 문자열을 생성하는 모델을 확인해보도록 하겠습니다. 이 스크립트에서 하는 작업은 이전 단어가 주어졌을 때 현재 단어가 나올 확률을 예측하는 것입니다. 지금은 단어가 아닌 각각의 글자 단위로 언어 모델을 구축해보겠습니다. \n",
    "\n",
    "사용할 데이터는 `이상한 나라의 엘리스` 문장입니다. 주어진 텍스트에서 10개의 글자를 입력으로 받아서 다음 글자를 예측하는 모델을 만드는 것을 목적으로 합니다. 단어 단위가 아닌 글자 단위로 학습을 하는 이유는 단어의 갯수보다 글자의 갯수가 적으므로 학습을 더 빠르게 진행할 수 있기 때문입니다. 워크샵에서 단어 단위의 모델을 형성하는 것에는 시간이 오래 걸릴 수 있으므로, 글자 단위의 모델을 만들어 보겠습니다. \n",
    "\n",
    "모델이 잘 학습이 된다면 생성된 결과도 좋을 것이고, 모델이 잘 학습이 되지 않을 경우 이상한 문장들을 생성해낼 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the data\n",
    "\n",
    "데이터를 확인해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "head: cannot open './data/11-0.txt' for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head ./data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
