{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P.O.S tagging using Gated Recurrent Unit (GRU)\n",
    "\n",
    "`GRU` (Cho et al., 2014) 구조는 간단하게는 이전에 사용하였던 `LSTM`에서 최종 결과물을 산출하는 `gate`를 제외하였다고 이해할 수 있습니다. 그렇기 때문에 `LSTM`과 비슷하게 `vanishing gradient effect`를 방지할 수 있으면서 연산 속도에서도 이득을 볼 수 있는 구조입니다. 이번에는 `GRU`를 이용하여 자동으로 품사 태깅을 수행하는 모델을 만들어 보도록 하겠습니다. \n",
    "\n",
    "**Note:** 아래 내용을 실행하기 전, **seq2seq** 구조를 다시 한 번 설명하고 진행하겠습니다. \n",
    "\n",
    "**Note:** 전체 코드는 [이 코드](https://github.com/PacktPublishing/Deep-Learning-with-Keras/blob/master/Chapter06/pos_tagging_gru.py)를 토대로 하였으며, 필요에 따라 변경하였습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules\n",
    "\n",
    "필요한 모듈들을 불러오겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Activation, Dense, RepeatVector, SpatialDropout1D\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "from keras.layers.recurrent import GRU, LSTM\n",
    "\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nltk\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(87)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "학습을 위해서는 품사 정보가 태깅되어 있는 학습 데이터가 필요합니다. `NLTK`를 통해서 Penn Treebank의 10% 샘플을 사용할 수 있습니다. 이 데이터를 사용해서 training 및 test를 진행하겠습니다. 아래 명령어를 사용해서 treebank 데이터를 다운받겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 -m nltk.downloader -d /usr/local/share/nltk_data treebank punkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`treebank` 데이터는 `nltk` 툴을 이용해서 파싱된 형태로 읽어올 수 있습니다. 다음의 `bash` 명령어를 이용해서 treebank를 저장할 폴더를 만들고, 그 이후의 `python` 코드를 이용해서 treebank의 `문장과 태깅`을 서로 다른 파일에 저장하도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'data/treebank': File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir data/treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_data = open(\"data/treebank/treebank_sent.txt\", \"w\")\n",
    "pos_data = open(\"data/treebank/treebank_pos.txt\", \"w\")\n",
    "\n",
    "treebank_data = nltk.corpus.treebank.tagged_sents()\n",
    "\n",
    "for sent in treebank_data:\n",
    "    words, poss = [], []\n",
    "    for word, pos in sent:\n",
    "        if pos == \"-NONE-\":\n",
    "            continue\n",
    "        words.append(word)\n",
    "        poss.append(pos)\n",
    "    sent_data.write(\"{:s}\\n\".format(\" \".join(words)))\n",
    "    pos_data.write(\"{:s}\\n\".format(\" \".join(poss)))\n",
    "sent_data.close()\n",
    "pos_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일이 제대로 작성되었는지 아래 명령어를 통해서 확인해보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
      "Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group .\n",
      "Rudolph Agnew , 55 years old and former chairman of Consolidated Gold Fields PLC , was named a nonexecutive director of this British industrial conglomerate .\n",
      "A form of asbestos once used to make Kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than 30 years ago , researchers reported .\n",
      "The asbestos fiber , crocidolite , is unusually resilient once it enters the lungs , with even brief exposures to it causing symptoms that show up decades later , researchers said .\n",
      "Lorillard Inc. , the unit of New York-based Loews Corp. that makes Kent cigarettes , stopped using crocidolite in its Micronite cigarette filters in 1956 .\n",
      "Although preliminary findings were reported more than a year ago , the latest results appear in today 's New England Journal of Medicine , a forum likely to bring new attention to the problem .\n",
      "A Lorillard spokewoman said , `` This is an old story .\n",
      "We 're talking about years ago before anyone heard of asbestos having any questionable properties .\n",
      "There is no asbestos in our products now . ''\n",
      "==================================================================\n",
      "NNP NNP , CD NNS JJ , MD VB DT NN IN DT JJ NN NNP CD .\n",
      "NNP NNP VBZ NN IN NNP NNP , DT NNP VBG NN .\n",
      "NNP NNP , CD NNS JJ CC JJ NN IN NNP NNP NNP NNP , VBD VBN DT JJ NN IN DT JJ JJ NN .\n",
      "DT NN IN NN RB VBN TO VB NNP NN NNS VBZ VBN DT JJ NN IN NN NNS IN DT NN IN NNS VBN TO PRP RBR IN CD NNS IN , NNS VBD .\n",
      "DT NN NN , NN , VBZ RB JJ IN PRP VBZ DT NNS , IN RB JJ NNS TO PRP VBG NNS WDT VBP RP NNS JJ , NNS VBD .\n",
      "NNP NNP , DT NN IN JJ JJ NNP NNP WDT VBZ NNP NNS , VBD VBG NN IN PRP$ NN NN NNS IN CD .\n",
      "IN JJ NNS VBD VBN RBR IN DT NN IN , DT JJS NNS VBP IN NN POS NNP NNP NNP IN NNP , DT NN JJ TO VB JJ NN TO DT NN .\n",
      "DT NNP NN VBD , `` DT VBZ DT JJ NN .\n",
      "PRP VBP VBG IN NNS IN IN NN VBD IN NN VBG DT JJ NNS .\n",
      "EX VBZ DT NN IN PRP$ NNS RB . ''\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "head data/treebank/treebank_sent.txt\n",
    "echo \"==================================================================\"\n",
    "head data/treebank/treebank_pos.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "미리 설정할 수 있는 `hyperparameter`를 미리 정의하도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_file = \"data/treebank/treebank_sent.txt\"\n",
    "pos_file = \"data/treebank/treebank_pos.txt\"\n",
    "\n",
    "embed_dim = 300  # embedding layer에서 사용할 feature의 수\n",
    "hidden_dim = 128  # GRU 마지막 결과값이 전달될 fully-connected layer의 노드 갯수\n",
    "batch_size = 32  # 한 번에 처리할 데이터의 수\n",
    "num_epoch = 2# 전체 데이터를 반복할 횟수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other parameters\n",
    "\n",
    "이전 `CNN`에서와 마찬가지로 다른 `parameter`를 찾아보도록 하겠습니다. `CNN`에서와 마찬가지로 다음의 정보가 필요합니다. \n",
    "\n",
    " 1. 사용된 단어의 수\n",
    " 2. 가장 긴 문장의 길이\n",
    " 3. 단어-인덱스 관계를 정의한 `<dict>` \n",
    " 4. 인덱스-단어 관계를 정의한 `<dict>`\n",
    " 5. 품사-인덱스 관계를 정의한 `<dict>` \n",
    " 6. 인덱스-품사 관계를 정의한 `<dict>`\n",
    " \n",
    "이전과 다르게 단어만 다루는 것이 아니라 단어의 `품사`도 다루기 때문에 두 개의 `<dict>` 변수가 더 필요하게 되었습니다. \n",
    "\n",
    "필요한 정보를 다음의 코드들을 이용해서 찾아보도록 하겠습니다. 두 개의 파일에 대해서 같은 작업을 진행할 예정이므로, 함수를 정의하여서 두 개의 데이터에 대해서 같은 작업을 실행하도록 하겠습니다. \n",
    "\n",
    "**Note:** `python`의 `def` 기능을 잘 모르는 경우 간단히 설명하고 진행하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Info\n",
      "\tNumber of words:  10947\n",
      "\tMaximum length:  249\n",
      "\tNumber of sents:  3914\n",
      "========================================\n",
      "POS Info\n",
      "\tNumber of POS:  45\n",
      "\tMaximum length:  249\n",
      "\tNumber of sents:  3914\n"
     ]
    }
   ],
   "source": [
    "# Parse sentence\n",
    "\n",
    "def parse_sentence(filename):\n",
    "    counter = collections.Counter()\n",
    "    max_len = 0\n",
    "    num_sent = 0\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            words = line.strip().lower().split()\n",
    "\n",
    "            # get frequency\n",
    "            for word in words:\n",
    "                counter[word] += 1\n",
    "\n",
    "            # get max_len\n",
    "            if len(words) > max_len:\n",
    "                max_len = len(words)\n",
    "\n",
    "            # get num_sent\n",
    "            num_sent += 1\n",
    "\n",
    "        f.close()\n",
    "    return counter, max_len, num_sent\n",
    "\n",
    "sent_counter, sent_max_len, sent_num_sent = parse_sentence(sent_file)\n",
    "pos_counter, pos_max_len, pos_num_sent = parse_sentence(pos_file)\n",
    "\n",
    "print(\"Sentence Info\")\n",
    "print(\"\\tNumber of words: \", len(sent_counter))\n",
    "print(\"\\tMaximum length: \", sent_max_len)\n",
    "print(\"\\tNumber of sents: \", sent_num_sent)\n",
    "print(\"=\" * 40)\n",
    "print(\"POS Info\")\n",
    "print(\"\\tNumber of POS: \", len(pos_counter))\n",
    "print(\"\\tMaximum length: \", pos_max_len)\n",
    "print(\"\\tNumber of sents: \", pos_num_sent)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드를 통해 데이터에는 총 10,947개의 고유 단어가 있었고, 고유 품사는 45개 있음을 알 수 있습니다. 가장 긴 문장은 249개이며, 총 데이터 갯수는 3,914개입니다. 고유 단어와 고유 품사의 갯수는 다를 수 있지만, 나머지는 일치하여야 합니다. \n",
    "\n",
    "이전까지는 모든 단어를 사용하여 예측해보았기 때문에, 이제는 `<UNK>`를 활용하여서 모델을 구성해보도록 하겠습니다. 따라서 빈도수가 높은 상위 7000개의 단어를 사용하여 주어진 데이터를 학습하고, 품사는 모든 45개의 품사를 예측해보도록 하겠습니다. 가장 긴 문장의 길이는 250으로 설정하도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 250\n",
    "num_vocab = 7000\n",
    "num_pos = 45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 모델에서와 마찬가지로 단어-인덱스 관계와 품사-인덱스 관계, 그리고 그 역관계를 정의하는 `<dict>`가 필요합니다. 다음의 코드를 사용하여서 각각의 `<dict>`를 정의해보겠습니다. 같은 작업이 두 번 진행되므로 함수를 정의하여 사용하겠습니다. `PAD`와 `UNK`를 정의하기 위해 전체 단어/품사 갯수에 2를 더해주겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAD UNK\n",
      "PAD\n",
      "104 government\n",
      "3 nnp\n"
     ]
    }
   ],
   "source": [
    "def create_dictionary(counter_name, max_num, sentence = False):\n",
    "    # tmp_word2idx = collections.defaultdict(int)\n",
    "    if sentence:\n",
    "        tmp_word2idx = {x[0]: i + 2 for i, x in enumerate(counter_name.most_common(max_num))}\n",
    "        tmp_word2idx[\"PAD\"] = 0\n",
    "        tmp_word2idx[\"UNK\"] = 1\n",
    "    else:\n",
    "        tmp_word2idx = {x[0]: i + 1 for i, x in enumerate(counter_name.most_common(max_num))}\n",
    "        tmp_word2idx[\"PAD\"] = 0\n",
    "    tmp_idx2word = {v: k for k, v in tmp_word2idx.items()}\n",
    "    \n",
    "    return tmp_word2idx, tmp_idx2word\n",
    "\n",
    "sent_word2idx, sent_idx2word = create_dictionary(sent_counter, num_vocab, sentence = True)\n",
    "pos_word2idx, pos_idx2word = create_dictionary(pos_counter, num_pos)\n",
    "\n",
    "print(sent_idx2word[0], sent_idx2word[1])\n",
    "print(pos_idx2word[0])\n",
    "\n",
    "print(sent_word2idx[\"government\"], sent_idx2word[sent_word2idx[\"government\"]])\n",
    "print(pos_word2idx[\"nnp\"], pos_idx2word[pos_word2idx[\"nnp\"]])\n",
    "\n",
    "sent_num_vocab = num_vocab + 2\n",
    "pos_num_pos = num_pos + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수를 통해 필요한 `<dict>`들을 생성하였으며, 필요한 `PAD`와 `UNK`가 제대로 입력되어있음을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion\n",
    "\n",
    "이전과 마찬가지로 전체 문장을 `인덱스`의 나열로 변경하여 저장하여야 합니다. `seq2seq`에서 출력은 입력한 **sequence**를 토대로 다른 **sequence**를 출력해 내는 것입니다. 현재 품사 태깅에서는 **250**개의 단어를 입력으로 받아서 **250**개의 품사를 출력하는 `seq2seq` 모델을 만들게 되며, 출력에서 품사는 실제 품사가 아니라 `one-hot encodding`으로 표현이 된 후 `pos_idx2word`를 사용하여 실제 품사로 전환될 것입니다. \n",
    "\n",
    "다음의 코드를 이용해서 데이터를 변환하도록 하겠습니다. 이전에는 입력과 출력 데이터를 다루는 방법이 달랐기 때문에 각각 진행을 하였습니다. 지금은 비슷한 과정이 두 번 진행되어야 하기 때문에 함수를 이용해서 진행하도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3914, 250) (3914, 250, 46)\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0 6995 4728    2 2374   80  322\n",
      "    2   40 2631    3  123   23    7 2108  318  450 2227    4]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "104 government\n",
      "the second to last word of the first sentence:  29\n",
      "POS of that word:  cd\n"
     ]
    }
   ],
   "source": [
    "def build_tensor(filename, num_sent, word2idx, max_len, category = False):\n",
    "    data = np.empty((num_sent, ), dtype = list)\n",
    "    with open(filename, \"r\") as f:\n",
    "        sent_num = 0\n",
    "        for line in f:\n",
    "            wids = []\n",
    "            line = line.lower()\n",
    "            for word in line.strip().split():\n",
    "                try:\n",
    "                    wids.append(word2idx[word])\n",
    "                except KeyError:\n",
    "                    wids.append(word2idx[\"UNK\"])\n",
    "            #if category:\n",
    "            #    data[sent_num] = np_utils.to_categorical(wids, num_classes=len(word2idx))\n",
    "                # print(line)\n",
    "                # print(data[sent_num])\n",
    "            #else:\n",
    "            data[sent_num] = wids\n",
    "            sent_num += 1\n",
    "        f.close()\n",
    "    \n",
    "    tensor_data = sequence.pad_sequences(data, maxlen = max_len)\n",
    "    if category:\n",
    "        tensor_data = np_utils.to_categorical(tensor_data, num_classes=len(word2idx))\n",
    "    return tensor_data\n",
    "\n",
    "X_data = build_tensor(sent_file, sent_num_sent, sent_word2idx, seq_len)\n",
    "y_data = build_tensor(pos_file, pos_num_sent, pos_word2idx, seq_len, category = True)\n",
    "\n",
    "print(X_data.shape, y_data.shape)\n",
    "print(X_data[0])\n",
    "print(y_data[0][0])\n",
    "\n",
    "print(sent_word2idx[\"government\"], sent_idx2word[sent_word2idx[\"government\"]])\n",
    "\n",
    "print(\"the second to last word of the first sentence: \", sent_idx2word[X_data[0][248]])\n",
    "print(\"POS of that word: \", pos_idx2word[np.argmax(y_data[0][248])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종적으로 아래와 같이 데이터를 `training/test` 데이터로 분할합니다. 전체 데이터 중 20%가 test 데이터로 사용되었습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3131, 250) (783, 250) (3131, 250, 46) (783, 250, 46)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.2, random_state = 42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding layer\n",
    "\n",
    "word2vec과 같은 방법으로 사전 학습된 `embedding layer`를 만들 수도 있지만, 실제로는 word2vec과는 다른 방법으로 품사 태깅은 `embedding layer`가 만들어집니다. 여기에서는 무작위의 값으로 초기화된 `embedding layer`를 만든 이후, 모델의 학습 과정에서 `embedding layer`도 학습하도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model\n",
    "\n",
    "이제는 실제로 `seq2seq` 모델을 작성해보겠습니다. 먼저 입력값은 인덱스로 변환된 *(None x seq_len)* 형태의 데이터입니다. 이 데이터는 **embedding layer**를 거쳐서 *(None x seq_len x 128)* 로 변환이 됩니다(128은 사전에 `hyperparameter`로 설정해둔 **embed_dim**의 값입니다). embedding layer의 결과값은 **GRU** 구조를 통과하게 되고, 그 결과는 *(None x 64)* 의 형태로 나타납니다(64는 사전에 `hyperparameter`로 설정해둔 **hidden_dim**의 값입니다). `encoder-decoder`를 연결하는 **Repeat Vector** 레이어에 GRU 구조의 결과값이 입력되면  *(None x seq_len x 128)* 의 형태를 반환하고 그 결과는 다시 `decoder` 계층의 **GRU**를 역으로 통과하게 됩니다. decoder의 GRU를 통과한 값은 **fully-connected layer**로 연결되어 *(None x seq_len x pos_num_pos)* 의 값을 출력하게 됩니다. 최종 레이어의 값들은 `softmax` 활성화 함수를 사용하여 값이 산출되며, **argmax**의 인덱스를 이용하여 품사 정보를 예측하게 됩니다. \n",
    "\n",
    "다음과 같이 모델을 구성해보도록 하겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 250, 300)          2100600   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_5 (Spatial (None, 250, 300)          0         \n",
      "_________________________________________________________________\n",
      "gru_9 (GRU)                  (None, 128)               164736    \n",
      "_________________________________________________________________\n",
      "repeat_vector_5 (RepeatVecto (None, 250, 128)          0         \n",
      "_________________________________________________________________\n",
      "gru_10 (GRU)                 (None, 250, 128)          98688     \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 250, 46)           5934      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 250, 46)           0         \n",
      "=================================================================\n",
      "Total params: 2,369,958\n",
      "Trainable params: 2,369,958\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(sent_num_vocab, embed_dim, input_length = seq_len))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(GRU(hidden_dim, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model.add(RepeatVector(seq_len))\n",
    "model.add(GRU(hidden_dim, return_sequences = True))\n",
    "model.add(TimeDistributed(Dense(pos_num_pos)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluate the model\n",
    "\n",
    "실제 모델을 학습하도록 하겠습니다. 학습에 사용할 epoch 값은 보통 여러 숫자를 시도해본 이후 `오버피팅(over-fitting)`이 일어나지 않는 수준에서 정하는 것이 일반적입니다. 이번에는 2회 실시한 이후 값을 살펴보겠습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3131 samples, validate on 783 samples\n",
      "Epoch 1/2\n",
      "3131/3131 [==============================] - 101s 32ms/step - loss: 0.6056 - acc: 0.9043 - val_loss: 0.6030 - val_acc: 0.9019\n",
      "Epoch 2/2\n",
      "3131/3131 [==============================] - 91s 29ms/step - loss: 0.5897 - acc: 0.9043 - val_loss: 0.6000 - val_acc: 0.9019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f88e2ce4a90>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size = batch_size, epochs = num_epoch, validation_data = [X_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783/783 [==============================] - 5s 7ms/step\n",
      "Test score: 0.600, accuracy: 0.902\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(X_test, y_test, batch_size = batch_size)\n",
    "print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종적으로 모델은 최종적으로 90%정도 일치하는 출력물을 내놓았다는 것을 확인할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
